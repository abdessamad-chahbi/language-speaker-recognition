# ğŸ—£ï¸ Automatic Language & Speaker Recognition

**MFCC Feature Extraction + GMM Modeling**

## ğŸ“Œ Overview

This project implements a system capable of automatically identifying both the **language** and the **speaker** from an audio sample. It leverages **MFCC (Mel-Frequency Cepstral Coefficients)** for feature extraction and **Gaussian Mixture Models (GMM)** for classification.

The system supports **5 Languages**:  
ğŸ‡²ğŸ‡¦ **Arabic** | ğŸ‡¬ğŸ‡§ English | ğŸ‡«ğŸ‡· French | ğŸ‡¯ğŸ‡µ Japanese | ğŸ‡ªğŸ‡¸ Spanish  
and multiple speakers (male & female).

This work is structured in **two major parts**:

1. **Language Recognition**
2. **Speaker Recognition**

---

## ğŸ“˜ Table of Contents

- Project summary  
- Highlights   
- Methodology 
- Experimental Results  
- Tech Stack  
- System Architecture 
- Future Improvements

---

## ğŸ§© Project summary

This project implements an audio classification pipeline based on **MFCCs and GMMs**. It contains two complementary modules:

- **Part I â€” Language Recognition:** classify audio into one of five languages (Arabic, English, French, Japanese, Spanish) using MFCCs + GMMs.  
- **Part II â€” Speaker Recognition:** reorganize MFCC feature files per speaker, then train perâ€‘speaker GMMs to identify speakers.

## âœ¨ Highlights

- Feature extraction using MFCC (python_speech_features / librosa).  
- GMM classifiers tested with multiple component sizes (8, 16, 32, 64, 128).  
- Separate workflows for language classification and speaker identification.  
- Model persistence with joblib for later scoring.  
- Evaluation with accuracy, confusion matrices and score comparisons across GMM sizes.

---

## ğŸ§  Methodology (high level)

1. Preprocess audio â†’ compute MFCC feature vectors per file.  
2. For language recognition (Part I):
   - Aggregate MFCCs for each language.
   - Train GMM models per language with different numbers of components.
   - Score test files against each language GMM and assign the language with highest likelihood.  
3. For speaker recognition (Part II):
   - Reorganize MFCCs by language â†’ gender â†’ speaker_id â†’ train/test.
   - Train a GMM per speaker (varying components) and identify the best model per speaker.
   - Score test audio to find the most likely speaker model.  
4. Evaluate using accuracy and confusion matrix; analyze effect of GMM components on performance.

---

## âœ… Experimental Results

ğŸ“Œ Increasing the number of Gaussian components generally **improves performance**  
ğŸ“Œ Speaker recognition is more challenging â†’ intra-speaker variability must be considered  

âœ” High language classification accuracy  
âœ” Robust speaker recognition using trained GMMs  
âœ” Saved models for later deployment using `.joblib`  

---

## ğŸ› ï¸ Tech Stack

| Category          | Tools                             |
| ----------------- | --------------------------------- |
| Programming       | Python                            |
| Speech Processing | MFCC, python_speech_features      |
| ML                | Gaussian Mixture Models (Sklearn) |
| Storage           | Joblib                            |
| Data Handling     | NumPy, Pandas                     |
| Visualization     | Matplotlib                        |
| Development       | Jupyter Notebook                  |

---

## ğŸ“Š System Architecture

```
Audio Input â†’ MFCC Extraction â†’ GMM Language Models â†’ Predicted Language
                             â†“
                      GMM Speaker Models â†’ Predicted Speaker
```

---

## ğŸš€ Future Improvements

* Add **HMM / GMM-UBM** for speaker modeling
* Expand dataset with more speakers & languages
* Integrate real-time microphone input
* Deploy as a GUI or Web Application
* Experiment with **Deep Learning-based embeddings (x-vectors, wav2vec2.0)**

---
